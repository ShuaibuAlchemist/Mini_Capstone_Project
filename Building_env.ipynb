{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30880d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fea2f62",
   "metadata": {},
   "source": [
    "### Data Source Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d7c13",
   "metadata": {},
   "source": [
    "##### Price Data — Coingecko API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbb235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to data\\ethereum_usd_30d_daily_20250810_115622.joblib\n",
      "                  price\n",
      "timestamp              \n",
      "2025-07-12  2958.851445\n",
      "2025-07-13  2942.961606\n",
      "2025-07-14  2974.265495\n",
      "2025-07-15  3012.179707\n",
      "2025-07-16  3133.069858\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# === LOAD .env FILE ===\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GECKO_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API_KEY not found in .env file\")\n",
    "\n",
    "# === FUNCTION TO FETCH & SAVE DATA ===\n",
    "def fetch_and_save_coingecko_data(coin_id=\"ethereum\", vs_currency=\"usd\", days=\"7\", interval=\"daily\"):\n",
    "    url = f\"https://pro-api.coingecko.com/api/v3/coins/{coin_id}/market_chart\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x-cg-pro-api-key\": api_key\n",
    "    }\n",
    "    params = {\n",
    "        \"vs_currency\": vs_currency,\n",
    "        \"days\": days,\n",
    "        \"interval\": interval\n",
    "    }\n",
    "\n",
    "    # Fetch data\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
    "\n",
    "    data = response.json()\n",
    "    prices = data.get(\"prices\", [])\n",
    "    df = pd.DataFrame(prices, columns=[\"timestamp\", \"price\"])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    # Create dynamic filename\n",
    "    filename = f\"{coin_id}_{vs_currency}_{days}d_{interval}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n",
    "    filepath = os.path.join(\"data\", filename)\n",
    "\n",
    "    # Ensure folder exists\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    # Save file\n",
    "    joblib.dump(df, filepath)\n",
    "    print(f\"Data saved to {filepath}\")\n",
    "\n",
    "    return filepath\n",
    "\n",
    "# === FUNCTION TO LOAD SAVED DATA ===\n",
    "def load_saved_data(filepath):\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "# ==== EXAMPLE USAGE ====\n",
    "# Save new data\n",
    "file_path = fetch_and_save_coingecko_data(coin_id=\"ethereum\", days=\"30\", interval=\"daily\")\n",
    "\n",
    "# Load saved data later\n",
    "df_loaded = load_saved_data(file_path)\n",
    "print(df_loaded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752888b",
   "metadata": {},
   "source": [
    "##### Whale Activity — Dune Sim API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf3c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching whale data from Dune query 5601937...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "Waiting for results...\n",
      "✅ Data saved to: data/dune_whale_data\\whale_data_20250810_120118.csv\n",
      "   eth_amount                                from_address  \\\n",
      "0  830.995000  0x46340b20830761efd32832a74d7169b29feb9758   \n",
      "1  410.000000  0xdfd5293d8e347dfe59e90efd55b2956a1343963d   \n",
      "2  649.999993  0x8d07bb400c9d08dcf900ac61b832dbdc27ee41b9   \n",
      "3  649.999993  0x8d07bb400c9d08dcf900ac61b832dbdc27ee41b9   \n",
      "4  649.999993  0x8d07bb400c9d08dcf900ac61b832dbdc27ee41b9   \n",
      "\n",
      "                     timestamp                                  to_address  \\\n",
      "0  2025-08-10 10:57:59.000 UTC  0xeae7380dd4cef6fbd1144f49e4d1e6964258a4f4   \n",
      "1  2025-08-10 10:55:47.000 UTC  0x6455327f820edd69c4cd665b995e0fec679d7f9e   \n",
      "2  2025-08-10 10:54:11.000 UTC  0x1a34f1b1860885f6b0946847894719711e43ca2c   \n",
      "3  2025-08-10 10:54:11.000 UTC  0x1a34f1b1860885f6b0946847894719711e43ca2c   \n",
      "4  2025-08-10 10:54:11.000 UTC  0x1a34f1b1860885f6b0946847894719711e43ca2c   \n",
      "\n",
      "                                             tx_hash     usd_value  \n",
      "0  0x9f08d6d313e939c0f6e13219ccf53bc142354727fe83...  2.247426e+05  \n",
      "1  0x339544231d86f8f6f7c06729621e53a652fd40e4d655...  1.108845e+05  \n",
      "2  0x4b17a561facbf9165e2caf05a088971a1728e1d0974a...  1.758575e+05  \n",
      "3  0x4b17a561facbf9165e2caf05a088971a1728e1d0974a...  1.169545e+05  \n",
      "4  0x4b17a561facbf9165e2caf05a088971a1728e1d0974a...  2.734244e+06  \n"
     ]
    }
   ],
   "source": [
    "#Extract on-chain large transfers > $100K for the same token and timeframe (e.g., Ethereum, past 7 days).\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# Configurable Parameters\n",
    "# =========================\n",
    "QUERY_ID = 5601937  # Dune query ID for whale transfers\n",
    "OUTPUT_DIR = \"data/dune_whale_data\"  # where CSV files will be stored\n",
    "SAVE_FILENAME = f\"whale_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "# =========================\n",
    "# Setup\n",
    "# =========================\n",
    "load_dotenv()\n",
    "DUNE_API_KEY = os.getenv(\"DUNE_KEY\")  # Or use \"DUNE_API_KEY\" in your .env\n",
    "\n",
    "if not DUNE_API_KEY:\n",
    "    raise ValueError(\"DUNE_KEY not found in .env file\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "headers = {\n",
    "    \"x-dune-api-key\": DUNE_API_KEY,\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Helper Function to Run Query\n",
    "# =========================\n",
    "def fetch_dune_query(query_id):\n",
    "    \"\"\"Run a Dune query and return results as a DataFrame.\"\"\"\n",
    "    # Trigger execution\n",
    "    execution_url = f\"https://api.dune.com/api/v1/query/{query_id}/execute\"\n",
    "    exec_response = requests.post(execution_url, headers=headers)\n",
    "    exec_response.raise_for_status()\n",
    "    execution_id = exec_response.json()[\"execution_id\"]\n",
    "\n",
    "    # Poll for results\n",
    "    result_url = f\"https://api.dune.com/api/v1/execution/{execution_id}/results\"\n",
    "    while True:\n",
    "        res = requests.get(result_url, headers=headers)\n",
    "        res.raise_for_status()\n",
    "        data = res.json()\n",
    "        state = data.get(\"state\")\n",
    "\n",
    "        if state == \"QUERY_STATE_COMPLETED\":\n",
    "            break\n",
    "        elif state in [\"QUERY_STATE_FAILED\", \"QUERY_STATE_CANCELLED\"]:\n",
    "            raise RuntimeError(f\"Query failed or was cancelled: {data}\")\n",
    "        else:\n",
    "            print(\"Waiting for results...\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    results = data[\"result\"][\"rows\"]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# =========================\n",
    "# Main Script\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Fetching whale data from Dune query {QUERY_ID}...\")\n",
    "    df_whales = fetch_dune_query(QUERY_ID)\n",
    "\n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(OUTPUT_DIR, SAVE_FILENAME)\n",
    "    df_whales.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Data saved to: {output_path}\")\n",
    "\n",
    "    # Preview\n",
    "    print(df_whales.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
